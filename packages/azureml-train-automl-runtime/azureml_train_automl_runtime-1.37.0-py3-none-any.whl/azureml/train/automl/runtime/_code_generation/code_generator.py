# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------
"""Functions used to generate Python script for customer usage."""
import json
import os
import pickle
import shutil
import tempfile
from typing import Any, List, Optional, cast

from sklearn.pipeline import Pipeline

from azureml.core import Run

from azureml.automl.core import _codegen_utilities
from azureml.automl.core.shared._diagnostics.contract import Contract
from azureml.automl.runtime.shared.model_wrappers import (
    ForecastingPipelineWrapper,
    PreFittedSoftVotingClassifier,
    PreFittedSoftVotingRegressor,
    StackEnsembleBase,
)
from azureml.train.automl import _constants_azureml

from . import utilities
from .constants import FunctionNames
from .ensemble_model_template import EnsembleModelTemplate, StackEnsembleModelTemplate
from .featurizer_template import AbstractFeaturizerTemplate
from .model_template import AbstractModelTemplate, SingleModelTemplate
from .pipeline_step_template import PipelineStepTemplate
from .template_factory import featurizer_template_factory, preprocessor_template_factory


def _get_setup_run(parent_run: Run) -> Run:
    setup_run_list = list(parent_run._client.run.get_runs_by_run_ids(run_ids=[f"{parent_run.id}_setup"]))
    # if this is a local run there will be no setup iteration
    if len(setup_run_list) == 0:
        setup_run = parent_run
    else:
        setup_run = setup_run_list[0]
    return setup_run


def generate_full_script(child_run: Run, pipeline: Optional[Any] = None) -> str:
    with _codegen_utilities.use_custom_repr():
        output = [
            "# ---------------------------------------------------------",
            "# Copyright (c) Microsoft Corporation. All rights reserved.",
            "# ---------------------------------------------------------",
            "# This file has been autogenerated by the Azure Automated Machine Learning SDK.",
            "\n",
            "import numpy",
            "import pickle",
            "import logging",
            "import argparse"
            "\n",
            "logger = logging.getLogger('azureml.train.automl.runtime.codegen_script')",
            "\n",
            f"def {FunctionNames.SETUP_INSTRUMENTATION_FUNC_NAME}(run):",
            "    parent_run = run.parent",
            "    from azureml.automl.core.shared import logging_utilities, log_server",
            "    from azureml.telemetry import INSTRUMENTATION_KEY",
            "    try:",
            "        log_server.enable_telemetry(INSTRUMENTATION_KEY)",
            "        log_server.set_verbosity('INFO')",
            "        log_server.update_custom_dimensions({'codegen_run_id': run.id, 'parent_run_id': parent_run.id})",
            "    except Exception:",
            "        pass",
            "",
            "    logger.info(\"Instrumentation setup complete.\")",
            "\n"
        ]

        parent_run = child_run.parent
        properties = parent_run.properties

        training_dataset_id, validation_dataset_id = utilities.get_input_datasets(parent_run)

        settings_json = properties.get(_constants_azureml.Properties.AML_SETTINGS)
        settings_obj = json.loads(settings_json)

        task_type = cast(str, settings_obj.get("task_type"))
        label_column_name = cast(str, settings_obj.get("label_column_name"))
        weight_column_name = cast(str, settings_obj.get("weight_column_name"))
        is_timeseries = cast(bool, settings_obj.get("is_timeseries", False))

        tempdir = None

        if pipeline is None:
            try:
                tempdir = tempfile.mkdtemp()

                # Retrieve the preprocessor/algorithm sklearn pipeline
                child_run.download_file(_constants_azureml.MODEL_PATH, tempdir)
                pipeline_path = os.path.join(tempdir, os.path.basename(_constants_azureml.MODEL_PATH))
                with open(pipeline_path, "rb") as f:
                    pipeline = pickle.load(f)
            finally:
                if tempdir is not None:
                    shutil.rmtree(tempdir, ignore_errors=True)

        featurizer_template = featurizer_template_factory.select_template(pipeline, task_type)

        output.append(get_dataset_code(FunctionNames.GET_TRAIN_DATASET_FUNC_NAME, training_dataset_id))
        output.append("\n")
        validation_dataset_exists = validation_dataset_id is not None
        if validation_dataset_exists:
            output.append(get_dataset_code(FunctionNames.GET_VALID_DATASET_FUNC_NAME, str(validation_dataset_id)))
            output.append("\n")
        output.append(get_prepare_data_code(is_timeseries, label_column_name, weight_column_name))
        output.append("\n")

        output.append(get_model_code(pipeline, featurizer_template, is_timeseries=is_timeseries))
        output.append("\n")
        output.append(get_train_model_code())
        output.append("\n")
        output.append(get_scriptrun_code(is_timeseries, validation_dataset_exists))
        output.append("\n")
        output.append("if __name__ == '__main__':")
        output.append(get_dataset_parameter_code(training_dataset_id, validation_dataset_id))
        if validation_dataset_id is not None:
            output.append("    main(args.training_dataset_id, args.validation_dataset_id)")
        else:
            output.append("    main(args.training_dataset_id)")

        output_str = "\n".join(output)

        # Experimental auto formatting. We can't rely on this until black has a proper public API
        # See https://github.com/psf/black/issues/779
        # Maybe we can also add some automated cleanup of unused imports?
        # https://stackoverflow.com/questions/54325116/can-i-handle-imports-in-an-abstract-syntax-tree
        try:
            import black

            return black.format_str(output_str, mode=black.Mode())
        except Exception:
            return output_str


def get_dataset_parameter_code(training_dataset_id, validation_dataset_id):
    output = [
        "    parser = argparse.ArgumentParser()",
        f"parser.add_argument('--training_dataset_id', type=str, default=\'{training_dataset_id}\', \
help='Default training dataset id is populated from the parent run')"
    ]

    if validation_dataset_id is not None:
        output.append(
            f"parser.add_argument('--validation_dataset_id',  type=str, default=\'{validation_dataset_id}\', \
help='Default validation dataset id is populated from the parent run')")

    output.append("args = parser.parse_args()")
    output.append("")

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_dataset_code(dataset_func_name: str, dataset_id: str) -> str:
    output = [
        f"def {dataset_func_name}(dataset_id):",
        "from azureml.core import Dataset, Run",
        "",
        f"logger.info(\"Running {dataset_func_name}\")",
        "ws = Run.get_context().experiment.workspace",
        "dataset = Dataset.get_by_id(workspace=ws, id=dataset_id)",
        "return dataset.to_pandas_dataframe()",
    ]

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_prepare_data_code(
    is_timeseries: bool, label_column_name: str, weight_column_name: Optional[str] = None
) -> str:
    # TODO: make stuff from data_transformation.py publicly visible here
    #  such as label encoding, dropping NaN, etc
    output = [
        f"def {FunctionNames.PREPARE_DATA_FUNC_NAME}(dataframe):",
        "from azureml.automl.runtime import data_cleaning",
        "",
        f"logger.info(\"Running {FunctionNames.PREPARE_DATA_FUNC_NAME}\")",
        f"label_column_name = '{label_column_name}'",
        "",
        "# extract the features, target and sample weight arrays",
        "y_train = dataframe[label_column_name].values",
    ]

    if weight_column_name:
        output.append(f"class_weights_column_name = '{weight_column_name}'")
        output.append("X_train = dataframe.drop([label_column_name, class_weights_column_name], axis=1)")
        output.append("sample_weights = dataframe[class_weights_column_name].values")
    else:
        output.append("X_train = dataframe.drop([label_column_name], axis=1)")
        output.append("sample_weights = None")

    # TODO: Make this API public (#1277252)
    output.append(
        "X_train, y_train, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X_train, y_train, sample_weights,"
        f" is_timeseries={is_timeseries}, target_column=label_column_name)"
    )
    output.append("return X_train, y_train, sample_weights")

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_model_code(
    sklearn_pipeline: Pipeline,
    featurizer_template: AbstractFeaturizerTemplate,
    is_timeseries: bool,
) -> str:
    output = featurizer_template.generate_featurizer_code()
    model = sklearn_pipeline.steps[-1][1]
    if isinstance(model, (PreFittedSoftVotingRegressor, PreFittedSoftVotingClassifier)):
        model_template = EnsembleModelTemplate(model)  # type: AbstractModelTemplate
        output += model_template.generate_model_code()
        pipeline_step_templates = [featurizer_template, model_template]
    elif isinstance(model, StackEnsembleBase):
        model_template = StackEnsembleModelTemplate(model)
        output += model_template.generate_model_code()
        pipeline_step_templates = [featurizer_template, model_template]
    else:
        preproc_template = preprocessor_template_factory.select_template(sklearn_pipeline)
        model_template = SingleModelTemplate(model)
        output += preproc_template.generate_preprocessor_code()
        output += model_template.generate_model_code()
        pipeline_step_templates = [featurizer_template, preproc_template, model_template]

    if is_timeseries:
        Contract.assert_type(sklearn_pipeline, "sklearn_pipeline", expected_types=ForecastingPipelineWrapper)
        sklearn_pipeline = cast(ForecastingPipelineWrapper, sklearn_pipeline)
        timeseries_stddev = cast(float, sklearn_pipeline.stddev)
        output.append(get_build_timeseries_model_pipeline_code(timeseries_stddev, featurizer_template))
    else:
        output.append(get_build_model_pipeline_code_from_templates(pipeline_step_templates))

    return "\n".join(output)


def get_build_model_pipeline_code_from_templates(pipeline_step_templates: List[PipelineStepTemplate]) -> str:
    output = [f"def {FunctionNames.BUILD_MODEL_FUNC_NAME}():"]

    imports = [_codegen_utilities.get_import(Pipeline)]
    output.extend(_codegen_utilities.generate_import_statements(imports))
    output.append("")

    output.append(f"logger.info(\"Running {FunctionNames.BUILD_MODEL_FUNC_NAME}\")")
    output.append("pipeline = Pipeline(")
    output.append("    steps=[")

    for template in pipeline_step_templates:
        output.extend(_codegen_utilities.indent_lines(template.generate_pipeline_step(), 8))

    output.append("    ]")
    output.append(")")
    output.append("")
    output.append("return pipeline")

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_build_timeseries_model_pipeline_code(
    stddev: Optional[float], featurizer_template: AbstractFeaturizerTemplate
) -> str:
    output = [f"def {FunctionNames.BUILD_MODEL_FUNC_NAME}():"]

    imports = [
        _codegen_utilities.get_import(Pipeline),
        _codegen_utilities.get_import(ForecastingPipelineWrapper),
    ]
    output.extend(_codegen_utilities.generate_import_statements(imports))
    output.append("")

    output.append(f"logger.info(\"Running {FunctionNames.BUILD_MODEL_FUNC_NAME}\")")
    output.append("pipeline = Pipeline(")
    output.append("    steps=[")
    output.extend(_codegen_utilities.indent_lines(featurizer_template.generate_pipeline_step(), 8))
    output.append(f"        ('model', {FunctionNames.MODEL_FUNC_NAME}())")
    output.append("    ]")
    output.append(")")
    output.append(f"forecast_pipeline_wrapper = ForecastingPipelineWrapper(pipeline, stddev={stddev})")
    output.append("")
    output.append("return forecast_pipeline_wrapper")

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_train_model_code() -> str:
    output = [
        f"def {FunctionNames.TRAIN_MODEL_FUNC_NAME}(X, y, sample_weights):",
        f"logger.info(\"Running {FunctionNames.TRAIN_MODEL_FUNC_NAME}\")",
        f"model_pipeline = {FunctionNames.BUILD_MODEL_FUNC_NAME}()",
        "",
        "model = model_pipeline.fit(X, y)",
        "return model",
    ]

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_scriptrun_code(is_timeseries: bool, validation_dataset_exists: bool) -> str:
    output = []
    if validation_dataset_exists:
        output.append("def main(training_dataset_id=None, validation_dataset_id=None):")
    else:
        output.append("def main(training_dataset_id=None):")

    output.extend(
        [
            "# The following code is for when running this code as part of an AzureML script run.",
            "from azureml.core import Run",
            "run = Run.get_context()",
            f"{FunctionNames.SETUP_INSTRUMENTATION_FUNC_NAME}(run)"
            "",
            f"df = {FunctionNames.GET_TRAIN_DATASET_FUNC_NAME}(training_dataset_id)",
            f"X, y, sample_weights = {FunctionNames.PREPARE_DATA_FUNC_NAME}(df)",
            f"model = {FunctionNames.TRAIN_MODEL_FUNC_NAME}(X, y, sample_weights)",
            "",
        ]
    )

    if validation_dataset_exists:
        output.extend(
            [
                f"valid_df = {FunctionNames.GET_VALID_DATASET_FUNC_NAME}(validation_dataset_id)",
                f"X_valid, y_valid, sample_weights_valid = {FunctionNames.PREPARE_DATA_FUNC_NAME}(valid_df)",
                "y_pred = model.{}(X_valid)".format("forecast" if is_timeseries else "predict"),
                "",
            ]
        )
    else:
        output.extend(
            [
                # TODO: this is predicting on the same data on the input (just to validate the model)
                # if validation dataset is not provided.
                #  Change this to use proper splitting.
                "y_pred = model.{}(X)".format("forecast" if is_timeseries else "predict"),
                "",
            ]
        )

    # TODO: Scoring (we need to handle the validation dataset)
    # Need to fill out the parameters
    """
    if task_type == constants.Tasks.CLASSIFICATION:
        output.extend([
            "from azureml.automl.runtime.shared.score.scoring import score_classification",
            "y_pred = model.predict(X_test)",
            "metrics = score_classification()"
        ])
    elif task_type == constants.Tasks.REGRESSION:
        output.extend([
            "from azureml.automl.runtime.shared.score.scoring import score_regression",
            "y_pred = model.predict(X_test)",
            "metrics = score_regression()"
        ])
    elif task_type == constants.Tasks.FORECASTING:
        output.extend([
            "from azureml.automl.runtime.shared.score.scoring import score_forecasting",
            "y_pred = model.forecast(X_test)",
            "metrics = score_forecasting()"
        ])
    else:
        # Other tasks are not implemented
        pass
    """

    # TODO: Emit code to log metrics

    output.extend(
        [
            "with open('model.pkl', 'wb') as f:",
            "    pickle.dump(model, f)",
            "run.upload_file('outputs/model.pkl', 'model.pkl')",
        ]
    )

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)

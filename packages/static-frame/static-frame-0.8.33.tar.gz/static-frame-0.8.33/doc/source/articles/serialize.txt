


----

Not supported in Pandas.to_parquet()
    Must have string column names
    Name attributes on Frames

From: https://ursalabs.org/blog/2020-feather-v2/
Parquet format has become one of the “gold standard” binary file formats for data warehousing

Parquet docs
https://arrow.apache.org/docs/python/parquet.html


# notes on changing limit on file descriptors
https://wilsonmar.github.io/maximum-limits/
# PROTIP: On MacOS, the maximum number that can be specified is 12288.
# ulimit -n

https://numpy.org/neps/nep-0001-npy-format.html

https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04

https://towardsdatascience.com/how-fast-is-reading-parquet-file-with-arrow-vs-csv-with-pandas-2f8095722e94

Parquet is based on Dremel from 2010
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36632.pdf

Record shredding: is it important?
https://www.joekearney.co.uk/posts/understanding-record-shredding



----
Title:

Using the NPY Format for Faster-Than Parquet, Memory-Mappable, Complete DataFrame Serialization

Employing NumPy's NPY & NPZ File Formats for Faster-than Parquet, Memory-Mappable, Complete DataFrame Serialization

Employing NumPy's NPY Format for Faster-Than-Parquet DataFrame Serialization

----

Abstract:
    You can use Markdown here. Please do not include any personally identifiable information. The initial round of reviews are anonymous, and this field will be visible to reviewers. Please write at most 300 words.


Over 14 years ago the first NumPy Enhancement Proposal (NEP) defined the NPY format (a binary encoding of array data and metadata) and the NPZ format (zipped bundles of NPY files). Those same formats, extended in a custom NPZ packaged with JSON metadata, can be used in Python to create a stable DataFrame storage format that can materially out-perform Parquet read / write times in a wide range of contexts. Unlike Parquet, all characteristics of a DataFrame can be encoded and all NumPy dtypes are supported. Implemented in StaticFrame, this format can take advantage of an immutable data model to memory-map full DataFrames from un-zipped directories of NPY. Given wide-spread use of Parquet files in data science workflows and AWS services such as Glue, a faster-than-parquet file format can significantly reduce compute costs. This talk will review the specification, implementation, and performance characteristics of this format.



Description:
    You can use Markdown here. Please do not include any personally identifiable information. The initial round of reviews are anonymous, and this field will be visible to reviewers. This section is also used on the schedule for attendees to view. Be clear and precise when describing your presentation. Please write at most 300 words.


I will begin this talk by introducing the challenge of serializing DataFrames, illustrating how nearly all stable encoding formats lack full support for all DataFrame characteristics. While the broadly-used Parquet format has been called a "gold standard" binary file format, its columnar representation will be shown to have limitations when used for encoding DataFrames.

I will show how the NPY format, combined with JSON metadata, can be used to create a custom NPZ file with significant performance and compatibility advantages compared to Parquet. The details of this encoding scheme will be explained.

I will close the talk by evaluating numerous read / write performance comparisons between Parquet (via Pandas) and NPZ (via StaticFrame), measured with a wide variety of DataFrame shapes and dtype compositions. I will share techniques used in implementing optimized Python routines for reading and writing NPY files, and demonstrate applications for memory-mapping complete DataFrames via the same NPY representation.



Audience
    1–2 paragraphs that should answer three questions: (1) Whom is this presentation for? (2) What background, knowledge, or experience do you expect attendees to have? (3) What do you expect attendees to learn, or to be able to do after attending your presentation?


This presentation is for anyone working in Python with DataFrames (including but not limited to Pandas DataFrames). Those working in NumPy, or using the Parquet format independent of Pandas, will also gain insight into approaches to serializing array data to disk. Those using AWS services such as Glue might also benefit from learning about options beyond Parquet for DataFrame serialization. This talk will assume basic experience with Pandas or related DataFrame libraries. Additional experience with Parquet files is beneficial but not required.

After attending my presentation, attendees will have a complete understanding of DataFrame characteristics and learn how various encoding formats, including Parquet, do not support all of those characteristics. Attendees will learn the history of the NPY format, its low-level specification, and optimizations found in a new implementation of encoders and decoders. Finally, attendees will learn how a custom NPZ format can encode an entire DataFrame, when, why, and by how much this format can outperform Parquet, and how the NPY format offers opportunities for memory mapping a complete DataFrame.





Outline



The challenge of serializing DataFrames
    DataFrames are more than just a table
        Index and columns have types, hierarchies, and name attributes
        Tables themseves might have name attributes or other metadata
    Common formats leave out information
        XLSX: no place for name of index and name of columns
        Database tables: no place for non-string column headers
    Pickling a DataFrame is always fastest
        Pickles not suitable for long-term storage
        Not secure for sharing
    DataFrame enciding and decoding performance is critical to common workflows

NPY, NPZ, & Parquet
    NPY Proposed as part of NEP 1 in 2007
    NPY simple binary format
        array metadata encoded in binary
        array data as contiguous bytes
    NPZ bundles multiple NPZ in a zipped
    NumPy provides bulit-in support: np.save, np.savez, np.load
    NPY supports Structured Arrays for differene columnar types
    NPY support object dtypes through pickling
        Pickles introduce compatibility and security issues
        Can pickle an array directly, to no advantage of NPY
    NPY & NPZ in era of DataFrames
        Despite performance, little widespread use
        CSV, XLSX more commonly used
        No support in Pandas
        Parquet becomes the "gold standard" format
            AWS Glue, Athena, Redshift
    The Rise of Parquet
        Parquet developed out of the Arrow project
        Parquet offers columnar encoding scheme
        Designed for more than tables, including nested structures
        Designed for multi-language support
        Provides one of the fastest formats for serializing DataFrames
        Limitations
            Columns labels can only be strings
            Indices and columns cannot be hierarchical
            No support for additional table metadata
            Not a one-to-one mapping to NumPy dtypes

Using NPY & NPZ to completely encode a DataFrame
    Extending the NPZ format
        Store all Frame and Index components as array NPY files
            Use common naming templates for file names
            Retain block structure
        Store metadata in a JSON file
    DataFrames can store 1D columns and 2D arrays of same-typed columns
        Pandas collects 2D arrays independent of column order
        StaticFrame permits adjacent columns of the same type to be 2D arrays
        By encoding larger 2D arrays, we can gain a performance advantage
    Encding metadata in JSON
        number of blocks
        index and columns depth
        name attributes
        index and columns types
    Encoding values
        Store each block as an NPY
        Store each underlying Index array for index and columns
    Encoding columns and index
        Store underlying arrays
        Do not store the underlying mapping: upfront creation cost

Getting NPY to be faster than Parquet
    NumPy's save / load routines emphasize compatibility, re-written for speed
    Caching NPY metadata

    Sources of Performance
        Homogenized adjacent data enabling 2D arrays
        Single read, no-copy loading
        One-to-one type system




I. The challenge of serializing DataFrames (10 min).
A. The importance of DataFrame encoding and decoding performance.
B. DataFrames are more than just a table.
C. How common encodings formats (CSV, XLSX, HDF5, Parquet) leave out information.
D. Pickling a DataFrame is always fastest, but not secure.

II. NPY & NPZ (5 min).
A. The origin of NPY in NEP 1.
B. Specification of the NPY and NPZ formats.
C. Why NPY & NPZ were not previously used for DataFrames.

III. Using NPY & NPZ to completely encode a DataFrame (10 min).
A. Extending the NPZ format with JSON metadata.
B. Encoding the values, index, and columns.

IV. Performance (5 min).
A. Enhancements to NPY encoding / decoding.
B. Comprehensive DataFrame performance comparisons between NPZ and Parquet.
C. Why NPZ can be faster than Parquet.



Past Experience

    Please summarize your teaching or public speaking experience, as well as your experience with the subject. Provide links to one (or two!) previous presentations by each speaker. If you have any additional notes, they can be placed here as well.

I have presented at numerous national and international conferences in many domains over the last twenty years, and taught as a university professor of music technology for six years, frequently teaching technical topics. I have never presented at a PyCon.

I have been programming in Python since the year 2000, I writing production Python for financial systems for nearly ten years, and I am expert in NumPy, Pandas, StaticFrame, and DataFrame libraries in general.


Examples of recent presentations:

- PyData Global 2021: "Why Datetimes Need Units: Avoiding a Y2262 Problem & Harnessing the Power of NumPy's datetime64": https://zoom.us/rec/share/MhHxZLi-SMkU3Sewhv7MKLWhgS0y0T7E7xFqAWfukUNdUGtFJFcHxJf8g2r_dTqq.cBJaD2SZP5P7eLI9?startTime=1635534301000

- PyData LA 2019: "Fitting Many Dimensions into One: The Promise of Hierarchical Indices for Data Beyond Two Dimensions": https://youtu.be/xX8tXSNDpmE




